#Only use this to install package as it is not directly available
cran <- getOption("repos")
cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"
options(repos = cran)
install.packages("mxnet")
library('mxnet')



train <- data.matrix(read.csv('~\\Statistics\\Kaggle\\Digit\\train.csv', header=T))
test <- data.matrix(read.csv('~\\Statistics\\Kaggle\\Digit\\test.csv', header=T))


#Function to view image in row
view_row <- function(row, data = train) {
  m = matrix(unlist(data[row,-1]),nrow = 28,byrow = T)
  m = apply(m, 2, rev)
  image(t(m),col=grey.colors(255))
}
view_row(3, data = test)

#Create function to view results vs image
plotResults <- function(images, preds){
  op <- par(no.readonly=TRUE)
  x <- ceiling(sqrt(length(images)))
  par(mfrow=c(x,x), mar=c(.1,.1,.1,.1))
  
  for (i in images){
    m <- matrix(test[i,], nrow=28, byrow=TRUE)
    m <- apply(m, 2, rev)
    image(t(m), col=grey.colors(255), axes=FALSE)
    text(0.05,0.1,col="white", cex=1.8, preds[i])
  }
  par(op)
}


#Split training data into features and labels
train.x <- train[,-1]
train.y <- train[,1]  

#Normalize data
train.x <- t(train.x/255)
test.x <- t(test/255)

#Build CNN

m2.data <- mx.symbol.Variable("data")

# 1st convolutional layer
m2.conv1 <- mx.symbol.Convolution(m2.data, kernel=c(5,5), num_filter=16)
m2.bn1 <- mx.symbol.BatchNorm(m2.conv1)
m2.act1 <- mx.symbol.Activation(m2.bn1, act_type="relu")
m2.pool1 <- mx.symbol.Pooling(m2.act1, pool_type="max", kernel=c(2,2), stride=c(2,2))
m2.drop1 <- mx.symbol.Dropout(m2.pool1, p=0.5)

# 2nd convolutional layer
m2.conv2 <- mx.symbol.Convolution(m2.drop1, kernel=c(3,3), num_filter=32)
m2.bn2 <- mx.symbol.BatchNorm(m2.conv2)
m2.act2 <- mx.symbol.Activation(m2.bn2, act_type="relu")
m2.pool2 <- mx.symbol.Pooling(m2.act2, pool_type="max", kernel=c(2,2), stride=c(2,2))
m2.drop2 <- mx.symbol.Dropout(m2.pool2, p=0.5)
m2.flatten <- mx.symbol.Flatten(m2.drop2)

# 4 Fully Connected layers
m2.fc1 <- mx.symbol.FullyConnected(m2.flatten, num_hidden=2048)
m2.act3 <- mx.symbol.Activation(m2.fc1, act_type="relu")

m2.fc2 <- mx.symbol.FullyConnected(m2.act3, num_hidden=1024)
m2.act4 <- mx.symbol.Activation(m2.fc2, act_type="relu")

m2.fc3 <- mx.symbol.FullyConnected(m2.act4, num_hidden=512)
m2.act5 <- mx.symbol.Activation(m2.fc3, act_type="relu")

m2.fc4 <- mx.symbol.FullyConnected(m2.act4, num_hidden=256)
m2.act6 <- mx.symbol.Activation(m2.fc3, act_type="relu")

m2.fc5 <- mx.symbol.FullyConnected(m2.act5, num_hidden=10)
m2.softmax <- mx.symbol.SoftmaxOutput(m2.fc5)

train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test.x
dim(test.array) <- c(28, 28, 1, ncol(test.x))

log <- mx.metric.logger$new() 
tick <- proc.time() 
mx.set.seed(0)

m2 <- mx.model.FeedForward.create(m2.softmax, 
                                  X = train.array, 
                                  y = train.y,
                                  num.round = 150, #May take a few hours if this is increased
                                  array.batch.size = 500,
                                  array.layout="colmajor",
                                  learning.rate = 0.01,
                                  momentum = 0.91,
                                  wd = 0.00001,
                                  eval.metric = mx.metric.accuracy,
                                  initializer = mx.init.uniform(0.07),
                                  epoch.end.callback = mx.callback.log.train.metric(1, log)
)


print(paste("Training took:", round((proc.time() - tick)[3],2),"seconds"))

m2.preds <- predict(m2, test.array)
m2.preds.value <- max.col(t(m2.preds)) - 1

plot(log$train, type="l", col="red", xlab="Iteration", ylab="Accuracy")


plotResults(1:64, m2.preds.value)

submission <- data.frame(ImageId=1:ncol(test.x), Label=m2.preds.value)
write.csv(submission, file = '~\\Statistics\\Kaggle\\Digit\\submission.csv', row.names = FALSE)
