{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing on Restaurant Reviews\n",
    "\n",
    "In this Jupyter notebook, we will carry out some natural language processing on a dataset containing reviews of a certain restaurant. The dataset was obtained on Kirill Eremenko's machine learning website https://www.superdatascience.com/machine-learning/\n",
    "\n",
    "The aim of this NLP task is to classify reviews based on if the reviewer liked the restaurant or not. We will create a Bag-Of-Words model which will break down sentences into its constituent terms. It will then analyze the frequency of used terms along with our target variable, whether or not a review liked the restaurant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Libraries and Data\n",
    "\n",
    "First we will load in the required libraries and data. After the standard numpy, pandas, and matplotlib are read in, libraries dedicated to NLP are loaded. These libraries will help us accomplish tasks such as forming a corpus out of our dataset, removing often used words (stopwords), removing whitespace/punctuation/contractions, and stemming words to their morphological base (loved -> love).\n",
    "\n",
    "This clean-up of the data is necessary to prevent unique words from being double counted. For example, many people may use the past tense of a word while others use a present tense. Using the porter stemmer will ensure that these words will be grouped together for further classification. \n",
    "\n",
    "After the data is cleaned and preprocessed, we will use the naive bayes and random forest algorithms to classify whether a consumer liked the restaurant or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "#Only need this for the first time\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a snapshot of our data to see what kind of reviews were left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n",
      "5     Now I am getting angry and I want my damn pho.      0\n",
      "6              Honeslty it didn't taste THAT fresh.)      0\n",
      "7  The potatoes were like rubber and you could te...      0\n",
      "8                          The fries were great too.      1\n",
      "9                                     A great touch.      1\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will define a function which will iterate through our data set. This iteration will accomplish the following:\n",
    "\n",
    "1. Extract the words used in each sentence, ignoring numbers\n",
    "2. Change all the words to lowercase\n",
    "3. Split the sentence into its constituent words\n",
    "4. Stem the words to their morphological base\n",
    "5. Remove 'stopwords'\n",
    "6. Re-attach our split words back into one string\n",
    "7. Create sparse matrix of word frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrepareData(dataset):\n",
    "    corpus = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        ps = PorterStemmer()\n",
    "        review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    \n",
    "    # Creating the Bag of Words model\n",
    "    cv = CountVectorizer(max_features = 1500)\n",
    "    X = cv.fit_transform(corpus).toarray()\n",
    "    y = dataset.iloc[:, 1].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = PrepareData(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We will be using two of the most common classification algorithms used in NLP, Naive Bayes and Random Forest. We end up with a similar precision, recall, and f score for both these classifiers. However, after implementing a cross validated grid search, I was able to edge out the naive bayes classifier by a little. \n",
    "\n",
    "Overall there I believe there is still some wiggle room for improvement for these classifiers. However, as this is my first NLP project I am satisfied with the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train/Test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.57      0.67        97\n",
      "          1       0.68      0.88      0.77       103\n",
      "\n",
      "avg / total       0.75      0.73      0.72       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_samples_split': 4, 'n_estimators': 1500}\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "params = {'n_estimators' : [500, 1000, 1500],\n",
    "          'max_depth': [ 2, 4, 6, 8],\n",
    "          'min_samples_split' : [2, 3,4,5]}\n",
    "\n",
    "cv = GridSearchCV(clf, param_grid = params, scoring = 'f1', cv = 5)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.93      0.78        97\n",
      "          1       0.89      0.56      0.69       103\n",
      "\n",
      "avg / total       0.78      0.74      0.73       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
